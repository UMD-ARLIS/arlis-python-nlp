{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIIDStuff.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIbD3l2-My6Q"
      },
      "outputs": [],
      "source": [
        "#https://huggingface.co/joeddav/xlm-roberta-large-xnli?text=Can+you+please+amend+the+invoice+to+reflect+true+capital+expenditure+and+anticipated+revenue%3F&candidate_labels=Medical%2Cbusiness%2Cfinance&multi_class=true\n",
        "\n",
        "# CUDA 11.1\n",
        "# Site to visit for later version: https://pytorch.org/get-started/previous-versions/\n",
        "!pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k\n",
        "\n",
        "import transformers\n",
        "from transformers import pipeline\n",
        "\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "wlkqzaPBQRJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_Sector_of_Deployment(text):\n",
        "  sectorDeployment = ['Information and communication', 'Arts, entertainment and recreation', 'Transportation and storage', 'Public administration and defence', 'Administrative and support service activities', 'Human health and social work activities', 'Education', 'Professional, scientific and technical activities', 'Financial and insurance activities', 'Wholesale and retail trade', 'Activities of households as employers', 'Accommodation and food service activities']\n",
        "  vector = classifier(text, sectorDeployment)\n",
        "  return vector['labels'][0]\n",
        "\n",
        "def get_infrastructure_sector(text):\n",
        "  infrastructureSector = ['Transportation', 'Healthcare and public health', 'Government facilities', 'Communications', 'Food and agriculture', 'Critical manufacturing', 'Nuclear', 'Financial services', 'Information technology']\n",
        "  vector = classifier(text, infrastructureSector)\n",
        "  return vector['labels'][0]\n",
        "\n",
        "def get_harm_type(text):\n",
        "  harmType = ['Harm to social or political systems', 'Harm to civil liberties', 'Harm to physical health/safety', 'Psychological harm', 'Financial harm', 'Harm to physical property', 'Harm to intangible property', 'Other:Harm to publicly available information', 'Other:Reputational harm; False incarceration', 'Other:Reputational harm', 'Other:Privacy', 'Other', 'Other:Reputational harm/social harm (libel and defamation)']\n",
        "  vector = classifier(text, harmType)\n",
        "  return vector['labels'][0]\n",
        "\n",
        "#Code taken from Lucas Ou-Yang  'Copyright 2014, Lucas Ou-Yang'\n",
        "# -*- coding: utf-8 -*-\n",
        "__title__ = 'newspaper'\n",
        "__author__ = 'Lucas Ou-Yang'\n",
        "__license__ = 'MIT'\n",
        "__copyright__ = 'Copyright 2014, Lucas Ou-Yang'\n",
        "\n",
        "import logging\n",
        "import copy\n",
        "import os\n",
        "import glob\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "from . import images\n",
        "from . import network\n",
        "from . import nlp\n",
        "from . import settings\n",
        "from . import urls\n",
        "\n",
        "from .cleaners import DocumentCleaner\n",
        "from .configuration import Configuration\n",
        "from .extractors import ContentExtractor\n",
        "from .outputformatters import OutputFormatter\n",
        "from .utils import (URLHelper, RawHelper, extend_config,\n",
        "                    get_available_languages, extract_meta_refresh)\n",
        "from .videos.extractors import VideoExtractor\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class ArticleDownloadState(object):\n",
        "    NOT_STARTED = 0\n",
        "    FAILED_RESPONSE = 1\n",
        "    SUCCESS = 2\n",
        "\n",
        "\n",
        "class ArticleException(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "class Article(object):\n",
        "    \"\"\"Article objects abstract an online news article page\n",
        "    \"\"\"\n",
        "    def __init__(self, url, title='', source_url='', config=None, **kwargs):\n",
        "        \"\"\"The **kwargs argument may be filled with config values, which\n",
        "        is added into the config object\n",
        "        \"\"\"\n",
        "        if isinstance(title, Configuration) or \\\n",
        "                isinstance(source_url, Configuration):\n",
        "            raise ArticleException(\n",
        "                'Configuration object being passed incorrectly as title or '\n",
        "                'source_url! Please verify `Article`s __init__() fn.')\n",
        "\n",
        "        self.config = config or Configuration()\n",
        "        self.config = extend_config(self.config, kwargs)\n",
        "\n",
        "        self.extractor = ContentExtractor(self.config)\n",
        "\n",
        "        if source_url == '':\n",
        "            scheme = urls.get_scheme(url)\n",
        "            if scheme is None:\n",
        "                scheme = 'http'\n",
        "            source_url = scheme + '://' + urls.get_domain(url)\n",
        "\n",
        "        if source_url is None or source_url == '':\n",
        "            raise ArticleException('input url bad format')\n",
        "\n",
        "        # URL to the main page of the news source which owns this article\n",
        "        self.source_url = source_url\n",
        "\n",
        "        self.url = urls.prepare_url(url, self.source_url)\n",
        "\n",
        "        self.title = title\n",
        "\n",
        "        # URL of the \"best image\" to represent this article\n",
        "        self.top_img = self.top_image = ''\n",
        "\n",
        "        # stores image provided by metadata\n",
        "        self.meta_img = ''\n",
        "\n",
        "        # All image urls in this article\n",
        "        self.imgs = self.images = []\n",
        "\n",
        "        # All videos in this article: youtube, vimeo, etc\n",
        "        self.movies = []\n",
        "\n",
        "        # Body text from this article\n",
        "        self.text = ''\n",
        "\n",
        "        # `keywords` are extracted via nlp() from the body text\n",
        "        self.keywords = []\n",
        "\n",
        "        # `meta_keywords` are extracted via parse() from <meta> tags\n",
        "        self.meta_keywords = []\n",
        "\n",
        "        # `tags` are also extracted via parse() from <meta> tags\n",
        "        self.tags = set()\n",
        "\n",
        "        # List of authors who have published the article, via parse()\n",
        "        self.authors = []\n",
        "\n",
        "        self.publish_date = ''\n",
        "\n",
        "        # Summary generated from the article's body txt\n",
        "        self.summary = ''\n",
        "\n",
        "        # This article's unchanged and raw HTML\n",
        "        self.html = ''\n",
        "\n",
        "        # The HTML of this article's main node (most important part)\n",
        "        self.article_html = ''\n",
        "\n",
        "        # Keep state for downloads and parsing\n",
        "        self.is_parsed = False\n",
        "        self.download_state = ArticleDownloadState.NOT_STARTED\n",
        "        self.download_exception_msg = None\n",
        "\n",
        "        # Meta description field in the HTML source\n",
        "        self.meta_description = \"\"\n",
        "\n",
        "        # Meta language field in HTML source\n",
        "        self.meta_lang = \"\"\n",
        "\n",
        "        # Meta favicon field in HTML source\n",
        "        self.meta_favicon = \"\"\n",
        "\n",
        "        # Meta site_name field in HTML source\n",
        "        self.meta_site_name = \"\"\n",
        "\n",
        "        # Meta tags contain a lot of structured data, e.g. OpenGraph\n",
        "        self.meta_data = {}\n",
        "\n",
        "        # The canonical link of this article if found in the meta data\n",
        "        self.canonical_link = \"\"\n",
        "\n",
        "        # Holds the top element of the DOM that we determine is a candidate\n",
        "        # for the main body of the article\n",
        "        self.top_node = None\n",
        "\n",
        "        # A deepcopied clone of the above object before heavy parsing\n",
        "        # operations, useful for users to query data in the\n",
        "        # \"most important part of the page\"\n",
        "        self.clean_top_node = None\n",
        "\n",
        "        # lxml DOM object generated from HTML\n",
        "        self.doc = None\n",
        "\n",
        "        # A deepcopied clone of the above object before undergoing heavy\n",
        "        # cleaning operations, serves as an API if users need to query the DOM\n",
        "        self.clean_doc = None\n",
        "\n",
        "        # A property dict for users to store custom data.\n",
        "        self.additional_data = {}\n",
        "\n",
        "    def build(self):\n",
        "        \"\"\"Build a lone article from a URL independent of the source (newspaper).\n",
        "        Don't normally call this method b/c it's good to multithread articles\n",
        "        on a source (newspaper) level.\n",
        "        \"\"\"\n",
        "        self.download()\n",
        "        self.parse()\n",
        "        self.nlp()\n",
        "\n",
        "    def _parse_scheme_file(self, path):\n",
        "        try:\n",
        "            with open(path, \"r\") as fin:\n",
        "                return fin.read()\n",
        "        except OSError as e:\n",
        "            self.download_state = ArticleDownloadState.FAILED_RESPONSE\n",
        "            self.download_exception_msg = e.strerror\n",
        "            return None\n",
        "\n",
        "    def _parse_scheme_http(self):\n",
        "        try:\n",
        "            return network.get_html_2XX_only(self.url, self.config)\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            self.download_state = ArticleDownloadState.FAILED_RESPONSE\n",
        "            self.download_exception_msg = str(e)\n",
        "            return None\n",
        "\n",
        "    def download(self, input_html=None, title=None, recursion_counter=0):\n",
        "        \"\"\"Downloads the link's HTML content, don't use if you are batch async\n",
        "        downloading articles\n",
        "\n",
        "        recursion_counter (currently 1) stops refreshes that are potentially\n",
        "        infinite\n",
        "        \"\"\"\n",
        "        if input_html is None:\n",
        "            parsed_url = urlparse(self.url)\n",
        "            if parsed_url.scheme == \"file\":\n",
        "                html = self._parse_scheme_file(parsed_url.path)\n",
        "            else:\n",
        "                html = self._parse_scheme_http()\n",
        "            if html is None:\n",
        "                log.debug('Download failed on URL %s because of %s' %\n",
        "                          (self.url, self.download_exception_msg))\n",
        "                return\n",
        "        else:\n",
        "            html = input_html\n",
        "\n",
        "        if self.config.follow_meta_refresh:\n",
        "            meta_refresh_url = extract_meta_refresh(html)\n",
        "            if meta_refresh_url and recursion_counter < 1:\n",
        "                return self.download(\n",
        "                    input_html=network.get_html(meta_refresh_url),\n",
        "                    recursion_counter=recursion_counter + 1)\n",
        "\n",
        "        self.set_html(html)\n",
        "        self.set_title(title)\n",
        "\n",
        "    def parse(self):\n",
        "        self.throw_if_not_downloaded_verbose()\n",
        "\n",
        "        self.doc = self.config.get_parser().fromstring(self.html)\n",
        "        self.clean_doc = copy.deepcopy(self.doc)\n",
        "\n",
        "        if self.doc is None:\n",
        "            # `parse` call failed, return nothing\n",
        "            return\n",
        "\n",
        "        # TODO: Fix this, sync in our fix_url() method\n",
        "        parse_candidate = self.get_parse_candidate()\n",
        "        self.link_hash = parse_candidate.link_hash  # MD5\n",
        "\n",
        "        document_cleaner = DocumentCleaner(self.config)\n",
        "        output_formatter = OutputFormatter(self.config)\n",
        "\n",
        "        title = self.extractor.get_title(self.clean_doc)\n",
        "        self.set_title(title)\n",
        "\n",
        "        authors = self.extractor.get_authors(self.clean_doc)\n",
        "        self.set_authors(authors)\n",
        "\n",
        "        meta_lang = self.extractor.get_meta_lang(self.clean_doc)\n",
        "        self.set_meta_language(meta_lang)\n",
        "\n",
        "        if self.config.use_meta_language:\n",
        "            self.extractor.update_language(self.meta_lang)\n",
        "            output_formatter.update_language(self.meta_lang)\n",
        "\n",
        "        meta_favicon = self.extractor.get_favicon(self.clean_doc)\n",
        "        self.set_meta_favicon(meta_favicon)\n",
        "\n",
        "        meta_site_name = self.extractor.get_meta_site_name(self.clean_doc)\n",
        "        self.set_meta_site_name(meta_site_name)\n",
        "\n",
        "        meta_description = \\\n",
        "            self.extractor.get_meta_description(self.clean_doc)\n",
        "        self.set_meta_description(meta_description)\n",
        "\n",
        "        canonical_link = self.extractor.get_canonical_link(\n",
        "            self.url, self.clean_doc)\n",
        "        self.set_canonical_link(canonical_link)\n",
        "\n",
        "        tags = self.extractor.extract_tags(self.clean_doc)\n",
        "        self.set_tags(tags)\n",
        "\n",
        "        meta_keywords = self.extractor.get_meta_keywords(\n",
        "            self.clean_doc)\n",
        "        self.set_meta_keywords(meta_keywords)\n",
        "\n",
        "        meta_data = self.extractor.get_meta_data(self.clean_doc)\n",
        "        self.set_meta_data(meta_data)\n",
        "\n",
        "        self.publish_date = self.extractor.get_publishing_date(\n",
        "            self.url,\n",
        "            self.clean_doc)\n",
        "\n",
        "        # Before any computations on the body, clean DOM object\n",
        "        self.doc = document_cleaner.clean(self.doc)\n",
        "\n",
        "        self.top_node = self.extractor.calculate_best_node(self.doc)\n",
        "        if self.top_node is not None:\n",
        "            video_extractor = VideoExtractor(self.config, self.top_node)\n",
        "            self.set_movies(video_extractor.get_videos())\n",
        "\n",
        "            self.top_node = self.extractor.post_cleanup(self.top_node)\n",
        "            self.clean_top_node = copy.deepcopy(self.top_node)\n",
        "\n",
        "            text, article_html = output_formatter.get_formatted(\n",
        "                self.top_node)\n",
        "            self.set_article_html(article_html)\n",
        "            self.set_text(text)\n",
        "\n",
        "        self.fetch_images()\n",
        "\n",
        "        self.is_parsed = True\n",
        "        self.release_resources()\n",
        "\n",
        "    def fetch_images(self):\n",
        "        if self.clean_doc is not None:\n",
        "            meta_img_url = self.extractor.get_meta_img_url(\n",
        "                self.url, self.clean_doc)\n",
        "            self.set_meta_img(meta_img_url)\n",
        "\n",
        "            imgs = self.extractor.get_img_urls(self.url, self.clean_doc)\n",
        "            if self.meta_img:\n",
        "                imgs.add(self.meta_img)\n",
        "            self.set_imgs(imgs)\n",
        "\n",
        "        if self.clean_top_node is not None and not self.has_top_image():\n",
        "            first_img = self.extractor.get_first_img_url(\n",
        "                self.url, self.clean_top_node)\n",
        "            if self.config.fetch_images:\n",
        "                self.set_top_img(first_img)\n",
        "            else:\n",
        "                self.set_top_img_no_check(first_img)\n",
        "\n",
        "        if not self.has_top_image() and self.config.fetch_images:\n",
        "            self.set_reddit_top_img()\n",
        "\n",
        "    def has_top_image(self):\n",
        "        return self.top_img is not None and self.top_img != ''\n",
        "\n",
        "    def is_valid_url(self):\n",
        "        \"\"\"Performs a check on the url of this link to determine if article\n",
        "        is a real news article or not\n",
        "        \"\"\"\n",
        "        return urls.valid_url(self.url)\n",
        "\n",
        "    def is_valid_body(self):\n",
        "        \"\"\"If the article's body text is long enough to meet\n",
        "        standard article requirements, keep the article\n",
        "        \"\"\"\n",
        "        if not self.is_parsed:\n",
        "            raise ArticleException('must parse article before checking \\\n",
        "                                    if it\\'s body is valid!')\n",
        "        meta_type = self.extractor.get_meta_type(self.clean_doc)\n",
        "        wordcount = self.text.split(' ')\n",
        "        sentcount = self.text.split('.')\n",
        "\n",
        "        if (meta_type == 'article' and len(wordcount) >\n",
        "                (self.config.MIN_WORD_COUNT)):\n",
        "            log.debug('%s verified for article and wc' % self.url)\n",
        "            return True\n",
        "\n",
        "        if not self.is_media_news() and not self.text:\n",
        "            log.debug('%s caught for no media no text' % self.url)\n",
        "            return False\n",
        "\n",
        "        if self.title is None or len(self.title.split(' ')) < 2:\n",
        "            log.debug('%s caught for bad title' % self.url)\n",
        "            return False\n",
        "\n",
        "        if len(wordcount) < self.config.MIN_WORD_COUNT:\n",
        "            log.debug('%s caught for word cnt' % self.url)\n",
        "            return False\n",
        "\n",
        "        if len(sentcount) < self.config.MIN_SENT_COUNT:\n",
        "            log.debug('%s caught for sent cnt' % self.url)\n",
        "            return False\n",
        "\n",
        "        if self.html is None or self.html == '':\n",
        "            log.debug('%s caught for no html' % self.url)\n",
        "            return False\n",
        "\n",
        "        log.debug('%s verified for default true' % self.url)\n",
        "        return True\n",
        "\n",
        "    def is_media_news(self):\n",
        "        \"\"\"If the article is related heavily to media:\n",
        "        gallery, video, big pictures, etc\n",
        "        \"\"\"\n",
        "        safe_urls = ['/video', '/slide', '/gallery', '/powerpoint',\n",
        "                     '/fashion', '/glamour', '/cloth']\n",
        "        for s in safe_urls:\n",
        "            if s in self.url:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def nlp(self):\n",
        "        \"\"\"Keyword extraction wrapper\n",
        "        \"\"\"\n",
        "        self.throw_if_not_downloaded_verbose()\n",
        "        self.throw_if_not_parsed_verbose()\n",
        "\n",
        "        nlp.load_stopwords(self.config.get_language())\n",
        "        text_keyws = list(nlp.keywords(self.text).keys())\n",
        "        title_keyws = list(nlp.keywords(self.title).keys())\n",
        "        keyws = list(set(title_keyws + text_keyws))\n",
        "        self.set_keywords(keyws)\n",
        "\n",
        "        max_sents = self.config.MAX_SUMMARY_SENT\n",
        "\n",
        "        summary_sents = nlp.summarize(title=self.title, text=self.text, max_sents=max_sents)\n",
        "        summary = '\\n'.join(summary_sents)\n",
        "        self.set_summary(summary)\n",
        "\n",
        "    def get_parse_candidate(self):\n",
        "        \"\"\"A parse candidate is a wrapper object holding a link hash of this\n",
        "        article and a final_url of the article\n",
        "        \"\"\"\n",
        "        if self.html:\n",
        "            return RawHelper.get_parsing_candidate(self.url, self.html)\n",
        "        return URLHelper.get_parsing_candidate(self.url)\n",
        "\n",
        "    def build_resource_path(self):\n",
        "        \"\"\"Must be called after computing HTML/final URL\n",
        "        \"\"\"\n",
        "        res_path = self.get_resource_path()\n",
        "        if not os.path.exists(res_path):\n",
        "            os.mkdir(res_path)\n",
        "\n",
        "    def get_resource_path(self):\n",
        "        \"\"\"Every article object has a special directory to store data in from\n",
        "        initialization to garbage collection\n",
        "        \"\"\"\n",
        "        res_dir_fn = 'article_resources'\n",
        "        resource_directory = os.path.join(settings.TOP_DIRECTORY, res_dir_fn)\n",
        "        if not os.path.exists(resource_directory):\n",
        "            os.mkdir(resource_directory)\n",
        "        dir_path = os.path.join(resource_directory, '%s_' % self.link_hash)\n",
        "        return dir_path\n",
        "\n",
        "    def release_resources(self):\n",
        "        # TODO: implement in entirety\n",
        "        path = self.get_resource_path()\n",
        "        for fname in glob.glob(path):\n",
        "            try:\n",
        "                os.remove(fname)\n",
        "            except OSError:\n",
        "                pass\n",
        "        # os.remove(path)\n",
        "\n",
        "    def set_reddit_top_img(self):\n",
        "        \"\"\"Wrapper for setting images. Queries known image attributes\n",
        "        first, then uses Reddit's image algorithm as a fallback.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            s = images.Scraper(self)\n",
        "            self.set_top_img(s.largest_image_url())\n",
        "        except TypeError as e:\n",
        "            if \"Can't convert 'NoneType' object to str implicitly\" in e.args[0]:\n",
        "                log.debug('No pictures found. Top image not set, %s' % e)\n",
        "            elif 'timed out' in e.args[0]:\n",
        "                log.debug('Download of picture timed out. Top image not set, %s' % e)\n",
        "            else:\n",
        "                log.critical('TypeError other than None type error. '\n",
        "                             'Cannot set top image using the Reddit '\n",
        "                             'algorithm. Possible error with PIL., %s' % e)\n",
        "        except Exception as e:\n",
        "            log.critical('Other error with setting top image using the '\n",
        "                         'Reddit algorithm. Possible error with PIL, %s' % e)\n",
        "\n",
        "    def set_title(self, input_title):\n",
        "        if input_title:\n",
        "            self.title = input_title[:self.config.MAX_TITLE]\n",
        "\n",
        "    def set_text(self, text):\n",
        "        text = text[:self.config.MAX_TEXT]\n",
        "        if text:\n",
        "            self.text = text\n",
        "\n",
        "    def set_html(self, html):\n",
        "        \"\"\"Encode HTML before setting it\n",
        "        \"\"\"\n",
        "        if html:\n",
        "            if isinstance(html, bytes):\n",
        "                html = self.config.get_parser().get_unicode_html(html)\n",
        "            self.html = html\n",
        "            self.download_state = ArticleDownloadState.SUCCESS\n",
        "\n",
        "    def set_article_html(self, article_html):\n",
        "        \"\"\"Sets the HTML of just the article's `top_node`\n",
        "        \"\"\"\n",
        "        if article_html:\n",
        "            self.article_html = article_html\n",
        "\n",
        "    def set_meta_img(self, src_url):\n",
        "        self.meta_img = src_url\n",
        "        self.set_top_img_no_check(src_url)\n",
        "\n",
        "    def set_top_img(self, src_url):\n",
        "        if src_url is not None:\n",
        "            s = images.Scraper(self)\n",
        "            if s.satisfies_requirements(src_url):\n",
        "                self.set_top_img_no_check(src_url)\n",
        "\n",
        "    def set_top_img_no_check(self, src_url):\n",
        "        \"\"\"Provide 2 APIs for images. One at \"top_img\", \"imgs\"\n",
        "        and one at \"top_image\", \"images\"\n",
        "        \"\"\"\n",
        "        self.top_img = src_url\n",
        "        self.top_image = src_url\n",
        "\n",
        "    def set_imgs(self, imgs):\n",
        "        \"\"\"The motive for this method is the same as above, provide APIs\n",
        "        for both `article.imgs` and `article.images`\n",
        "        \"\"\"\n",
        "        self.images = imgs\n",
        "        self.imgs = imgs\n",
        "\n",
        "    def set_keywords(self, keywords):\n",
        "        \"\"\"Keys are stored in list format\n",
        "        \"\"\"\n",
        "        if not isinstance(keywords, list):\n",
        "            raise Exception(\"Keyword input must be list!\")\n",
        "        if keywords:\n",
        "            self.keywords = keywords[:self.config.MAX_KEYWORDS]\n",
        "\n",
        "    def set_authors(self, authors):\n",
        "        \"\"\"Authors are in [\"firstName lastName\", \"firstName lastName\"] format\n",
        "        \"\"\"\n",
        "        if not isinstance(authors, list):\n",
        "            raise Exception(\"authors input must be list!\")\n",
        "        if authors:\n",
        "            self.authors = authors[:self.config.MAX_AUTHORS]\n",
        "\n",
        "    def set_summary(self, summary):\n",
        "        \"\"\"Summary here refers to a paragraph of text from the\n",
        "        title text and body text\n",
        "        \"\"\"\n",
        "        self.summary = summary[:self.config.MAX_SUMMARY]\n",
        "\n",
        "    def set_meta_language(self, meta_lang):\n",
        "        \"\"\"Save langauges in their ISO 2-character form\n",
        "        \"\"\"\n",
        "        if meta_lang and len(meta_lang) >= 2 and \\\n",
        "           meta_lang in get_available_languages():\n",
        "            self.meta_lang = meta_lang[:2]\n",
        "\n",
        "    def set_meta_keywords(self, meta_keywords):\n",
        "        \"\"\"Store the keys in list form\n",
        "        \"\"\"\n",
        "        self.meta_keywords = [k.strip() for k in meta_keywords.split(',')]\n",
        "\n",
        "    def set_meta_favicon(self, meta_favicon):\n",
        "        self.meta_favicon = meta_favicon\n",
        "\n",
        "    def set_meta_site_name(self, meta_site_name):\n",
        "        self.meta_site_name = meta_site_name\n",
        "\n",
        "    def set_meta_description(self, meta_description):\n",
        "        self.meta_description = meta_description\n",
        "\n",
        "    def set_meta_data(self, meta_data):\n",
        "        self.meta_data = meta_data\n",
        "\n",
        "    def set_canonical_link(self, canonical_link):\n",
        "        self.canonical_link = canonical_link\n",
        "\n",
        "    def set_tags(self, tags):\n",
        "        self.tags = tags\n",
        "\n",
        "    def set_movies(self, movie_objects):\n",
        "        \"\"\"Trim video objects into just urls\n",
        "        \"\"\"\n",
        "        movie_urls = [o.src for o in movie_objects if o and o.src]\n",
        "        self.movies = movie_urls\n",
        "\n",
        "    def throw_if_not_downloaded_verbose(self):\n",
        "        \"\"\"Parse ArticleDownloadState -> log readable status\n",
        "        -> maybe throw ArticleException\n",
        "        \"\"\"\n",
        "        if self.download_state == ArticleDownloadState.NOT_STARTED:\n",
        "            raise ArticleException('You must `download()` an article first!')\n",
        "        elif self.download_state == ArticleDownloadState.FAILED_RESPONSE:\n",
        "            raise ArticleException('Article `download()` failed with %s on URL %s' %\n",
        "                  (self.download_exception_msg, self.url))\n",
        "\n",
        "    def throw_if_not_parsed_verbose(self):\n",
        "        \"\"\"Parse `is_parsed` status -> log readable status\n",
        "        -> maybe throw ArticleException\n",
        "        \"\"\"\n",
        "        if not self.is_parsed:\n",
        "            raise ArticleException('You must `parse()` an article first!')\n",
        "\n",
        "\n",
        "    #Get Keywords\n",
        "    def keywords(text):\n",
        "    \"\"\"Get the top 10 keywords and their frequency scores ignores blacklisted\n",
        "    words in stopwords, counts the number of occurrences of each word, and\n",
        "    sorts them in reverse natural order (so descending) by number of\n",
        "    occurrences.\n",
        "    \"\"\"\n",
        "    NUM_KEYWORDS = 10\n",
        "    text = split_words(text)\n",
        "    # of words before removing blacklist words\n",
        "    if text:\n",
        "        num_words = len(text)\n",
        "        text = [x for x in text if x not in stopwords]\n",
        "        freq = {}\n",
        "        for word in text:\n",
        "            if word in freq:\n",
        "                freq[word] += 1\n",
        "            else:\n",
        "                freq[word] = 1\n",
        "\n",
        "        min_size = min(NUM_KEYWORDS, len(freq))\n",
        "        keywords = sorted(freq.items(),\n",
        "                          key=lambda x: (x[1], x[0]),\n",
        "                          reverse=True)\n",
        "        keywords = keywords[:min_size]\n",
        "        keywords = dict((x, y) for x, y in keywords)\n",
        "\n",
        "        for k in keywords:\n",
        "            articleScore = keywords[k] * 1.0 / max(num_words, 1)\n",
        "            keywords[k] = articleScore * 1.5 + 1\n",
        "        return dict(keywords)\n",
        "    else:\n",
        "        return dict()\n",
        "\n",
        "   \n",
        "    \n",
        "    #Location Finder\n",
        "    def locationFinder(text):\n",
        "        gpe = [] # countries, cities, states\n",
        "        loc = [] # non gpe locations, mountain ranges, bodies of water\n",
        "        doc = nlp(text)\n",
        "        for ent in doc.ents:\n",
        "          if (ent.label_ == 'GPE'):\n",
        "            gpe.append(ent.text)\n",
        "          elif (ent.label_ == 'LOC'):\n",
        "            loc.append(ent.text)\n",
        "        return gpe, loc\n",
        "\n",
        "    #Extraction people and organizations involved\n",
        "    def nameEntityFinder(paragraph):\n",
        "       doc = nlp(paragraph)\n",
        "       nameEntityDict = {}\n",
        "       nameEntityDict_v2 = {}\n",
        "       for ent in doc.ents:\n",
        "          nameEntityDict[ent.text] = ent.label_\n",
        "          \n",
        "       for (key, value) in nameEntityDict.items():\n",
        "           if value == 'PERSON' or value == 'ORG':\n",
        "               nameEntityDict_v2[key] = value\n",
        "       return nameEntityDict_v2\n",
        "    \n",
        "    #Returns pandas dataframe of all functions\n",
        "    def totalFunctions(url):\n",
        "      article = Article(url)\n",
        "      article.download()\n",
        "\n",
        "      article.parse() #parses through the text\n",
        "      article.nlp() \n",
        "      article.keywords\n",
        "      df = pd.DataFrame()\n",
        "      df['Function']=['Keywords', 'Author', 'Article Summary']\n",
        "      df['Result']=[article.keywords, article.authors, article.summary]\n",
        "      return df\n",
        "\n"
      ],
      "metadata": {
        "id": "UPMEEEJvNzR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article = Article('https://www.cbsnews.com/news/is-starbucks-shortchanging-its-baristas/')\n",
        "article.download()\n",
        "article.parse()\n",
        "\n",
        "get_Sector_of_Deployment(article.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "x2yGeSpwOmK9",
        "outputId": "1d021a47-d9fe-4dfb-867d-500339c4dad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Administrative and support service activities'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_infrastructure_sector(article.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PChjKB7aYNia",
        "outputId": "d6441c2c-5058-44e4-bc39-d58833666e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Food and agriculture'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_harm_type(article.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pi-NmxEzZrxa",
        "outputId": "91c9f619-a9c7-4a1c-efd4-c89dbbe25924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Other'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}